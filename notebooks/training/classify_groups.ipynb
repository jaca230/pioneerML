{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Classification (ZenML)\n",
    "\n",
    "Train the `GroupClassifier` on parquet-based time-group data:\n",
    "- Load groups from `ml_output_*.parquet` using the **C++ Arrow dataloader** (zero-copy)\n",
    "- Train via the new ZenML `group_classification_pipeline`\n",
    "- Export a TorchScript model (for C++/Python inference)\n",
    "\n",
    "**Note:** the model predicts a label *per time-group* (not per event). We aggregate to events later via the output adapter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ZenML repository root: /workspace\n",
      "Ensure this is the top-level of your repo (.zen must live here).\n",
      "\u001b[37mInitializing the ZenML global configuration version to 0.92.0\u001b[0m\n",
      "\u001b[37mCreating database tables\u001b[0m\n",
      "\u001b[37mCreating default project 'default' ...\u001b[0m\n",
      "\u001b[37mCreating default stack...\u001b[0m\n",
      "\u001b[33mThe current repo active project is no longer available.\u001b[0m\n",
      "\u001b[37mSetting the repo active project to 'default'.\u001b[0m\n",
      "\u001b[33mThe current repo active stack is no longer available. Resetting the active stack to default.\u001b[0m\n",
      "\u001b[37mSetting the global active project to 'default'.\u001b[0m\n",
      "\u001b[33mSetting the global active stack to default.\u001b[0m\n",
      "\u001b[37mReloading configuration file /workspace/.zen/config.yaml\u001b[0m\n",
      "ZenML ready with stack: default\n"
     ]
    }
   ],
   "source": [
    "from pioneerml.zenml import utils as zenml_utils\n",
    "\n",
    "PROJECT_ROOT = zenml_utils.find_project_root()\n",
    "zenml_client = zenml_utils.setup_zenml_for_notebook(root_path=PROJECT_ROOT, use_in_memory=True)\n",
    "print(f\"ZenML ready with stack: {zenml_client.active_stack_model.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "\n",
    "from pioneerml.zenml import load_step_output\n",
    "\n",
    "# Ensure the C++ dataloader Python bindings are on the path\n",
    "pml_bindings = Path(PROJECT_ROOT) / \"external\" / \"pioneerml_dataloaders\" / \"build\" / \"bindings\"\n",
    "sys.path.insert(0, str(pml_bindings))\n",
    "\n",
    "from pioneerml.zenml.pipelines.training import group_classification_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote small parquet: /workspace/artifacts/classify_groups_small.parquet (rows=8)\n"
     ]
    }
   ],
   "source": [
    "# Build a tiny parquet shard for quick debugging\n",
    "\n",
    "data_dir = Path(PROJECT_ROOT) / \"data\"\n",
    "src_parquet = data_dir / \"ml_output_000.parquet\"\n",
    "\n",
    "small_parquet = Path(PROJECT_ROOT) / \"artifacts\" / \"classify_groups_small.parquet\"\n",
    "small_rows = 8\n",
    "\n",
    "table = pq.read_table(src_parquet)\n",
    "table = table.slice(0, small_rows)\n",
    "small_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "pq.write_table(table, small_parquet)\n",
    "\n",
    "parquet_paths = [str(small_parquet)]\n",
    "print(f\"Wrote small parquet: {small_parquet} (rows={small_rows})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mInitiating a new run for the pipeline: \u001b[0m\u001b[38;5;105mgroup_classification_pipeline\u001b[37m.\u001b[0m\n",
      "\u001b[37mRegistered new pipeline: \u001b[0m\u001b[38;5;105mgroup_classification_pipeline\u001b[37m.\u001b[0m\n",
      "\u001b[37mCaching is disabled by default for \u001b[0m\u001b[38;5;105mgroup_classification_pipeline\u001b[37m.\u001b[0m\n",
      "\u001b[37mUsing user: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37mUsing stack: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  deployer: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  orchestrator: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  artifact_store: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37mYou can visualize your pipeline runs in the \u001b[0m\u001b[38;5;105mZenML Dashboard\u001b[37m. In order to try it locally, please run \u001b[0m\u001b[38;5;105mzenml login --local\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_group_classifier_data\u001b[37m has started.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_group_classifier_data\u001b[37m has finished in \u001b[0m\u001b[38;5;105m0.164s\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_group_classifier\u001b[37m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model   │ GroupClassifierStereo │  1.5 M │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ loss_fn │ BCEWithLogitsLoss     │      0 │ train │     0 │\n",
       "└───┴─────────┴───────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                 \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model   │ GroupClassifierStereo │  1.5 M │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ loss_fn │ BCEWithLogitsLoss     │      0 │ train │     0 │\n",
       "└───┴─────────┴───────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 1.5 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 1.5 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 6                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 56                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 1.5 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 1.5 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 6                                                                          \n",
       "\u001b[1mModules in train mode\u001b[0m: 56                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99cd6ace2034d5898ae830c185ba871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_group_classifier\u001b[37m has finished in \u001b[0m\u001b[38;5;105m2.098s\u001b[37m.\u001b[0m\n",
      "\u001b[37mPipeline run has finished in \u001b[0m\u001b[38;5;105m3.311s\u001b[37m.\u001b[0m\n",
      "Run name: group_classification_pipeline-2026_02_01-03_14_06_198794\n",
      "Run status: completed\n"
     ]
    }
   ],
   "source": [
    "# Run the new ZenML pipeline\n",
    "\n",
    "run = group_classification_pipeline.with_options(enable_cache=False)(\n",
    "    parquet_paths=parquet_paths,\n",
    "    config_json={\"time_window_ns\": 1.0},\n",
    "    max_epochs=1,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    ")\n",
    "print(f\"Run name: {run.name}\")\n",
    "print(f\"Run status: {run.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Events (graphs): 8\n",
      "Groups: 97\n",
      "x: (416, 4)\n",
      "edge_index: (2, 26378)\n",
      "edge_attr: (26378, 4)\n",
      "group_ptr: (9,)\n",
      "time_group_ids: (416,)\n",
      "logits: (97, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load artifacts and run a quick forward pass\n",
    "\n",
    "trained_module = load_step_output(run, \"train_group_classifier\")\n",
    "batch = load_step_output(run, \"load_group_classifier_data\")\n",
    "\n",
    "if trained_module is None or batch is None:\n",
    "    raise RuntimeError(\"Could not load artifacts from the group_classification_pipeline run.\")\n",
    "\n",
    "trained_module.eval()\n",
    "device = next(trained_module.parameters()).device\n",
    "data = batch.data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = trained_module.model(data)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Events (graphs): {data.num_graphs}\")\n",
    "print(f\"Groups: {data.num_groups}\")\n",
    "print(f\"x: {tuple(data.x.shape)}\")\n",
    "print(f\"edge_index: {tuple(data.edge_index.shape)}\")\n",
    "print(f\"edge_attr: {tuple(data.edge_attr.shape)}\")\n",
    "print(f\"group_ptr: {tuple(data.group_ptr.shape)}\")\n",
    "print(f\"time_group_ids: {tuple(data.time_group_ids.shape)}\")\n",
    "print(f\"logits: {tuple(logits.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript model: /workspace/trained_models/groupclassifier/groupclassifier_20260201_031410_torchscript.pt\n"
     ]
    }
   ],
   "source": [
    "# Export TorchScript model\n",
    "\n",
    "export_dir = Path(PROJECT_ROOT) / \"trained_models\" / \"groupclassifier\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "torchscript_path = export_dir / f\"groupclassifier_{stamp}_torchscript.pt\"\n",
    "\n",
    "trained_module.model.cpu()\n",
    "example = batch.data\n",
    "trained_module.model.export_torchscript(torchscript_path, example, strict=False)\n",
    "print(f\"Saved TorchScript model: {torchscript_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the TorchScript Model\n",
    "\n",
    "Export the trained model to TorchScript so it can be loaded in C++ or Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets sample: tensor([[1., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]]) tensor([[1., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Inspect a few target labels\n",
    "\n",
    "print(\"targets sample:\", batch.targets[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pioneerml)",
   "language": "python",
   "name": "pioneerml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
