{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca49c149",
   "metadata": {},
   "source": [
    "# PIONEER ML Tutorial\n",
    "\n",
    "Quick walkthrough of the pipeline framework, Lightning utilities, and how to plug in a model. This notebook builds a tiny synthetic dataset, wraps it in a DataModule, trains a model with the Lightning pipeline stage, and inspects results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40f0fc",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e7e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/jack/python_projects/pioneerML\n",
      "Added to sys.path: /home/jack/python_projects/pioneerML/src\n"
     ]
    }
   ],
   "source": [
    "import sys, pathlib\n",
    "\n",
    "def _find_repo_root(start: pathlib.Path) -> pathlib.Path:\n",
    "    for path in [start, *start.parents]:\n",
    "        if (path / 'pyproject.toml').exists():\n",
    "            return path\n",
    "    return start\n",
    "\n",
    "repo_root = _find_repo_root(pathlib.Path.cwd())\n",
    "src_dir = repo_root / 'src'\n",
    "for candidate in (repo_root, src_dir):\n",
    "    if str(candidate) not in sys.path:\n",
    "        sys.path.insert(0, str(candidate))\n",
    "\n",
    "print('Added to sys.path:', repo_root)\n",
    "print('Added to sys.path:', src_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396ac4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pioneerml.data import GraphGroupDataset\n",
    "from pioneerml.models import GroupClassifier\n",
    "from pioneerml.pipelines import Pipeline, Context, StageConfig\n",
    "from pioneerml.pipelines.stages import LightningTrainStage\n",
    "from pioneerml.training import GraphDataModule, GraphLightningModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b41dc3",
   "metadata": {},
   "source": [
    "## 2) Create a synthetic dataset\n",
    "We generate a few fake time-group records with the standardized per-hit fields expected by `GraphGroupDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fccf8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[8, 5], edge_index=[2, 56], edge_attr=[56, 4], y=[3], event_id=0, group_id=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_record(num_hits: int, event_id: int) -> dict:\n",
    "    coord = np.random.randn(num_hits).astype(np.float32)\n",
    "    z = np.random.randn(num_hits).astype(np.float32)\n",
    "    energy = np.abs(np.random.randn(num_hits)).astype(np.float32)\n",
    "    view = np.random.randint(0, 2, num_hits).astype(np.float32)\n",
    "\n",
    "    # Multi-label targets: [pion, muon, mip]\n",
    "    labels = [int(energy.mean() > 0.5), int(num_hits % 2 == 0)]\n",
    "    if len(labels) < 3:\n",
    "        labels.append(0)\n",
    "\n",
    "    return {\n",
    "        \"coord\": coord,\n",
    "        \"z\": z,\n",
    "        \"energy\": energy,\n",
    "        \"view\": view,\n",
    "        \"labels\": labels,\n",
    "        \"event_id\": event_id,\n",
    "        \"group_id\": event_id,\n",
    "    }\n",
    "\n",
    "records = [make_record(num_hits=8 + i, event_id=i) for i in range(20)]\n",
    "dataset = GraphGroupDataset(records, num_classes=3)\n",
    "dataset[0]  # trigger a build and inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5ed79",
   "metadata": {},
   "source": [
    "## 3) Wrap data with a Lightning DataModule\n",
    "Splits the dataset and prepares PyG loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da12c2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch_geometric.loader.dataloader.DataLoader at 0x7314f013e910>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7311d72a67d0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule = GraphDataModule(dataset=dataset, batch_size=4, val_split=0.2, test_split=0.0)\n",
    "datamodule.setup()\n",
    "datamodule.train_dataloader(), datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2f222",
   "metadata": {},
   "source": [
    "## 4) Build the model and Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669b3de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphLightningModule(\n",
       "  (model): GroupClassifier(\n",
       "    (input_embed): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x FullGraphTransformerBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): TransformerConv(64, 16, heads=4)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (jk): JumpingKnowledge(cat)\n",
       "    (pool): AttentionalAggregation(gate_nn=Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    ), nn=None)\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GroupClassifier(num_classes=3, hidden=64, num_blocks=2)\n",
    "lightning_module = GraphLightningModule(model, task=\"classification\", lr=1e-3)\n",
    "lightning_module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d8425",
   "metadata": {},
   "source": [
    "## 5) Compose a pipeline with the Lightning training stage\n",
    "The `LightningTrainStage` fits the module using any provided DataModule and records the trainer + trained module back into the shared `Context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b2d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "/home/jack/miniconda3/envs/pioneerml/lib/python3.11/site-packages/torch/cuda/__init__.py:230: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model   | GroupClassifier   | 118 K  | train\n",
      "1 | loss_fn | BCEWithLogitsLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.472     Total estimated model params size (MB)\n",
      "47        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb7c5f4a7ba4e61a8dadb2c2f7f723a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "[1/1] Stage 'train' failed: CUDA error: no kernel image is available for execution on the device\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m      1\u001b[39m train_stage = LightningTrainStage(\n\u001b[32m      2\u001b[39m     config=StageConfig(\n\u001b[32m      3\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m pipeline = Pipeline([train_stage], name=\u001b[33m\"\u001b[39m\u001b[33mtutorial_pipeline\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m ctx = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m ctx.summary()\n\u001b[32m     23\u001b[39m ctx.get(\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/pioneerML/src/pioneerml/pipelines/pipeline.py:223\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, context, skip_stages)\u001b[39m\n\u001b[32m    220\u001b[39m stage.validate_inputs(context)\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Execute\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stage._is_cleaned_up:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/pioneerML/src/pioneerml/pipelines/stages/model.py:185\u001b[39m, in \u001b[36mLightningTrainStage.execute\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    183\u001b[39m trainer_params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = params.get(\u001b[33m\"\u001b[39m\u001b[33mtrainer_params\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m    184\u001b[39m trainer = pl.Trainer(**trainer_params)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m context[\u001b[33m\"\u001b[39m\u001b[33mlightning_module\u001b[39m\u001b[33m\"\u001b[39m] = module\n\u001b[32m    188\u001b[39m context[\u001b[33m\"\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m\"\u001b[39m] = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:560\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:598\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    591\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    592\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    594\u001b[39m     ckpt_path,\n\u001b[32m    595\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    596\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    597\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1055\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1082\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1079\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:120\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;129m@_no_grad_context\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[_OUT_DICT]:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip:\n\u001b[32m    122\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:202\u001b[39m, in \u001b[36m_EvaluationLoop.setup_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28mself\u001b[39m._max_batches = []\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dl \u001b[38;5;129;01min\u001b[39;00m combined_loader.flattened:\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# determine number of batches\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     length = \u001b[38;5;28mlen\u001b[39m(dl) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_len_all_ranks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_zero_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    203\u001b[39m     limit_batches = \u001b[38;5;28mgetattr\u001b[39m(trainer, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlimit_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage.dataloader_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_batches\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m     num_batches = _parse_num_batches(stage, length, limit_batches)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:105\u001b[39m, in \u001b[36mhas_len_all_ranks\u001b[39m\u001b[34m(dataloader, strategy, allow_zero_length_dataloader_with_multiple_devices)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    104\u001b[39m total_length = strategy.reduce(torch.tensor(local_length, device=strategy.root_device), reduce_op=\u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtotal_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m:\n\u001b[32m    106\u001b[39m     rank_zero_warn(\n\u001b[32m    107\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal length of `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dataloader).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` across ranks is zero.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please make sure this was your intention.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m     )\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_length > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m local_length == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "train_stage = LightningTrainStage(\n",
    "    config=StageConfig(\n",
    "        name=\"train\",\n",
    "        params={\n",
    "            \"module\": lightning_module,\n",
    "            \"datamodule\": datamodule,\n",
    "            \"trainer_params\": {\n",
    "                \"max_epochs\": 2,\n",
    "                \"limit_train_batches\": 2,\n",
    "                \"limit_val_batches\": 1,\n",
    "                \"logger\": False,\n",
    "                \"enable_checkpointing\": False,\n",
    "                \"accelerator\": \"gpu\",   # required for CUDA\n",
    "                \"devices\": 1,           # pick GPU 0\n",
    "            },\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([train_stage], name=\"tutorial_pipeline\")\n",
    "ctx = pipeline.run(Context())\n",
    "ctx.summary()\n",
    "ctx.get(\"metrics\", {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cee97",
   "metadata": {},
   "source": [
    "## 6) Next steps\n",
    "- Swap in your own datasets or DataModules.\n",
    "- Add stages for preprocessing, evaluation, and checkpointing.\n",
    "- Integrate experiment tracking (e.g., Weights & Biases) by configuring the Lightning Trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
