{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb602a3-2ecb-428b-889b-efa3e3197ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /home/jack/python_projects/pioneerML\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Auto-detect project root by walking upward ---\n",
    "cwd = Path().resolve()\n",
    "ROOT = None\n",
    "\n",
    "for parent in [cwd] + list(cwd.parents):\n",
    "    if (parent / \"src\" / \"pioneerml\").exists():\n",
    "        ROOT = parent\n",
    "        break\n",
    "\n",
    "if ROOT is None:\n",
    "    raise RuntimeError(\"Could not find project root containing src/pioneerml\")\n",
    "\n",
    "sys.path.append(str(ROOT / \"src\"))\n",
    "print(\"Using project root:\", ROOT)\n",
    "\n",
    "# --- Normal imports ---\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pioneerml.data.datasets.graph_group import GraphRecord\n",
    "from pioneerml.training.datamodules.group import GroupClassificationDataModule\n",
    "from pioneerml.models.classifiers.group_classifier import GroupClassifier\n",
    "from pioneerml.training.lightning import GraphLightningModule\n",
    "from pioneerml.training.utils import default_precision_for_accelerator, set_tensor_core_precision\n",
    "from pioneerml.training.visualization import plot_loss_curves\n",
    "from pioneerml.training.utils import default_precision_for_accelerator\n",
    "from pioneerml.pipelines.stage import StageConfig\n",
    "from pioneerml.pipelines.stages.model import LightningTrainStage\n",
    "from pioneerml.pipelines.pipeline import Pipeline\n",
    "from pioneerml.pipelines.context import Context\n",
    "from pioneerml.training.progress import CleanProgressBar\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce8a911-3e8b-4ff1-be63-dc4100f5c93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda', '16-mixed')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    accelerator = \"cuda\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    precision = default_precision_for_accelerator(\"cuda\")\n",
    "    set_tensor_core_precision(\"medium\")\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    precision = \"32-true\"\n",
    "\n",
    "accelerator, precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8726f9-e14c-49b9-92ce-16eb30762918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_record(num_hits: int, event_id: int):\n",
    "    coord = np.random.randn(num_hits).astype(np.float32)\n",
    "    z = np.random.randn(num_hits).astype(np.float32)\n",
    "    energy = np.abs(np.random.randn(num_hits)).astype(np.float32)\n",
    "    view = np.random.randint(0, 2, size=num_hits).astype(np.float32)\n",
    "\n",
    "    energy_mean = energy.mean()\n",
    "    spatial_spread = coord.std()\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    # Energy: class 0 (high) or 1 (low)\n",
    "    if energy_mean > 1.0:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "    # Hits: class 2 (high) or 3 (low)\n",
    "    if num_hits > 20:\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        labels.append(3)\n",
    "\n",
    "    # Spread: class 4 (high) or 5 (low)\n",
    "    if spatial_spread > 1.0:\n",
    "        labels.append(4)\n",
    "    else:\n",
    "        labels.append(5)\n",
    "\n",
    "    return GraphRecord(\n",
    "        coord=coord,\n",
    "        z=z,\n",
    "        energy=energy,\n",
    "        view=view,\n",
    "        labels=labels,\n",
    "        event_id=event_id,\n",
    "        group_id=event_id,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16363308-c5fb-4ba7-823d-b0671ed7d0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = [make_synthetic_record(num_hits=np.random.randint(5, 40), event_id=i)\n",
    "           for i in range(600)]\n",
    "\n",
    "len(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d232b60-0e68-46fa-b6e5-7cc680d5c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = GroupClassificationDataModule(\n",
    "    records,\n",
    "    num_classes=3,\n",
    "    batch_size=32,\n",
    "    val_split=0.2,\n",
    "    test_split=0.0,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "dm.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72654902-df22-41bf-bf6b-bdba5d33e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Build model for 6 classes\n",
    "model = GroupClassifier(\n",
    "    hidden=128,\n",
    "    num_blocks=2,\n",
    "    num_classes=3,\n",
    ")\n",
    "\n",
    "# 2. Wrap it in the Lightning module\n",
    "lightning_module = GraphLightningModule(\n",
    "    model=model,\n",
    "    task=\"classification\",\n",
    "    lr=1e-3,\n",
    ")\n",
    "\n",
    "# 3. Build DataModule (you already have records)\n",
    "datamodule = GroupClassificationDataModule(\n",
    "    records,\n",
    "    num_classes=6,             # IMPORTANT for your new 6-class setup\n",
    "    batch_size=32,\n",
    "    val_split=0.2,\n",
    "    num_workers=4,             # or os.cpu_count()\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71d1b7b6-f1f5-4d37-b245-c8f5643a2f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name    | Type              | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model   | GroupClassifier   | 465 K  | train\n",
      "1 | loss_fn | BCEWithLogitsLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "465 K     Trainable params\n",
      "0         Non-trainable params\n",
      "465 K     Total params\n",
      "1.862     Total estimated model params size (MB)\n",
      "47        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[1/1] Stage 'train_synthetic_classifier' failed: Target size (torch.Size([64, 3])) must be the same as input size (torch.Size([32, 3]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64, 3])) must be the same as input size (torch.Size([32, 3]))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m pipeline = Pipeline(\n\u001b[32m     27\u001b[39m     stages=[train_stage],\n\u001b[32m     28\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33msynthetic_classification_pipeline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#Run pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m ctx = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/pioneerML/src/pioneerml/pipelines/pipeline.py:223\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, context, skip_stages)\u001b[39m\n\u001b[32m    220\u001b[39m stage.validate_inputs(context)\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# Execute\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stage._is_cleaned_up:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/pioneerML/src/pioneerml/pipelines/stages/model.py:190\u001b[39m, in \u001b[36mLightningTrainStage.execute\u001b[39m\u001b[34m(self, context)\u001b[39m\n\u001b[32m    188\u001b[39m     set_tensor_core_precision(params.get(\u001b[33m\"\u001b[39m\u001b[33mmatmul_precision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    189\u001b[39m trainer = pl.Trainer(**trainer_params)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m context[\u001b[33m\"\u001b[39m\u001b[33mlightning_module\u001b[39m\u001b[33m\"\u001b[39m] = module\n\u001b[32m    193\u001b[39m context[\u001b[33m\"\u001b[39m\u001b[33mtrainer\u001b[39m\u001b[33m\"\u001b[39m] = trainer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:560\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:598\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    591\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    592\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    594\u001b[39m     ckpt_path,\n\u001b[32m    595\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    596\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    597\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m   1055\u001b[39m         \u001b[38;5;28mself\u001b[39m.fit_loop.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1082\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1079\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m call._call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[39m, in \u001b[36m_no_grad_context.<locals>._decorator\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    177\u001b[39m     context_manager = torch.no_grad\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:145\u001b[39m, in \u001b[36m_EvaluationLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m    144\u001b[39m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:437\u001b[39m, in \u001b[36m_EvaluationLoop._evaluation_step\u001b[39m\u001b[34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[39m\n\u001b[32m    431\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m step_args = (\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mself\u001b[39m._build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[32m    436\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[32m    442\u001b[39m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:412\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_projects/pioneerML/src/pioneerml/training/lightning.py:78\u001b[39m, in \u001b[36mGraphLightningModule.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Batch, batch_idx: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     77\u001b[39m     preds, target = \u001b[38;5;28mself\u001b[39m._shared_step(batch)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     bs = \u001b[38;5;28mself\u001b[39m._get_batch_size(batch)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m.log(\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, loss, on_step=\u001b[38;5;28;01mFalse\u001b[39;00m, on_epoch=\u001b[38;5;28;01mTrue\u001b[39;00m, prog_bar=\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size=bs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/torch/nn/modules/loss.py:850\u001b[39m, in \u001b[36mBCEWithLogitsLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m    849\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/torch/nn/functional.py:3589\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3586\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m-> \u001b[39m\u001b[32m3589\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3590\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3591\u001b[39m     )\n\u001b[32m   3593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3594\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3595\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Target size (torch.Size([64, 3])) must be the same as input size (torch.Size([32, 3]))"
     ]
    }
   ],
   "source": [
    "# building training stage\n",
    "train_stage = LightningTrainStage(\n",
    "    config=StageConfig(\n",
    "        name=\"train_synthetic_classifier\",\n",
    "        params={\n",
    "            \"module\": lightning_module,\n",
    "            \"datamodule\": datamodule,\n",
    "            \"trainer_params\": {\n",
    "                \"accelerator\": \"auto\",            # GPU if available\n",
    "                \"devices\": 1,\n",
    "                \"max_epochs\": 10,\n",
    "                \"logger\": False,\n",
    "                \"enable_checkpointing\": False,\n",
    "                \"precision\": default_precision_for_accelerator(\"auto\"),\n",
    "                \"enable_model_summary\": True,\n",
    "                \"enable_progress_bar\": False,\n",
    "                \"callbacks\": [CleanProgressBar(bar_width=30)],\n",
    "            },\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Build pipeline with stages, this pipeline will just train so it will have one stage\n",
    "pipeline = Pipeline(\n",
    "    stages=[train_stage],\n",
    "    name=\"synthetic_classification_pipeline\",\n",
    ")\n",
    "\n",
    "#Run pipeline\n",
    "ctx = pipeline.run(Context())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ee83b-0d1f-4868-b584-4c194ffc3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get info from pipeline as context got filled out after training\n",
    "print(\"\\n=== PIPELINE CONTEXT SUMMARY ===\")\n",
    "print(ctx.summary())\n",
    "\n",
    "print(\"\\n=== TRAINING METRICS ===\")\n",
    "print(ctx.get(\"metrics\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87964a-c423-4779-bc51-4b5a153224a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(\n",
    "    lightning_module.train_epoch_loss_history,\n",
    "    lightning_module.val_epoch_loss_history,\n",
    "    title=\"Training vs Validation Loss\",\n",
    "    xlabel=\"Epoch\",\n",
    "    show=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f352a-691f-492c-a340-2efc5a49a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "truths = []\n",
    "\n",
    "trainer.model.eval()\n",
    "\n",
    "for batch in dm.val_dataloader():\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = trainer.model(batch)\n",
    "    preds.append(torch.sigmoid(out).cpu().numpy())\n",
    "    truths.append(batch.y.cpu().numpy())\n",
    "\n",
    "preds = np.vstack(preds)\n",
    "truths = np.vstack(truths)\n",
    "\n",
    "preds_binary = (preds > 0.5).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c927c1-6410-4144-8bfe-d6dbaba9121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parameters(record: GraphRecord):\n",
    "    coord = np.asarray(record.coord)\n",
    "    energy = np.asarray(record.energy)\n",
    "    return (\n",
    "        energy.mean(),\n",
    "        len(coord),\n",
    "        coord.std(),\n",
    "    )\n",
    "\n",
    "params = np.array([extract_parameters(r) for r in dm.val_dataset.indices])\n",
    "# Properly map val dataset indices back to original records\n",
    "val_records = [records[i] for i in dm.val_dataset.indices]\n",
    "params = np.array([extract_parameters(r) for r in val_records])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f6c1cd-8a96-466a-b441-95cbba211acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"energy_high\", \"nhits_high\", \"spread_high\"]\n",
    "thresholds = [1.0, 20, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12), sharex=True)\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    y_param = params[:, i]\n",
    "    pred = preds_binary[:, i]\n",
    "    truth = truths[:, i]\n",
    "\n",
    "    colors = [\"red\" if p == 0 else \"green\" for p in pred]\n",
    "    ax.scatter(np.arange(len(y_param)), y_param, c=colors, alpha=0.7)\n",
    "\n",
    "    ax.axhline(thresholds[i], color=\"black\", linestyle=\"--\", label=\"truth threshold\")\n",
    "\n",
    "    wrong = pred != truth\n",
    "    ax.scatter(np.where(wrong)[0], y_param[wrong], marker=\"x\", s=80, c=\"black\")\n",
    "\n",
    "    ax.set_ylabel(class_names[i])\n",
    "\n",
    "axes[-1].set_xlabel(\"sample index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
