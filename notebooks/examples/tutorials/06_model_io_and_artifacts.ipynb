{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3af30fe",
   "metadata": {},
   "source": [
    "# Tutorial 6: Saving and Exporting Models (LibTorch-friendly)\n",
    "\n",
    "Train a small tabular classifier, save/load its weights, and export a TorchScript artifact that can be loaded from C++ via LibTorch. ZenML is not required for these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63c92e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from pioneerml.pipelines.tutorial_examples.tabular_datamodule_pipeline import (\n",
    "    TabularConfig,\n",
    "    TabularClassifier,\n",
    "    TabularDataModule,\n",
    ")\n",
    "from pioneerml.common.zenml.utils import detect_available_accelerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f9eec",
   "metadata": {},
   "source": [
    "## 1) Train a small model (Lightning for convenience)\n",
    "\n",
    "We reuse the tabular DataModule/LightningModule to get a trained model quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec47dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | Sequential | 1.4 K  | train\n",
      "---------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m/home/jack/virtual_environments/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \u001b[0m\u001b[38;5;105mnum_workers\u001b[33m argument\u001b[0m\u001b[38;5;105m to \u001b[33mnum_workers=15\u001b[0m\u001b[38;5;105m in the \u001b[33mDataLoader` to improve performance.\n",
      "\u001b[0m\n",
      "\u001b[33m/home/jack/virtual_environments/miniconda3/envs/pioneerml/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \u001b[0m\u001b[38;5;105mnum_workers\u001b[33m argument\u001b[0m\u001b[38;5;105m to \u001b[33mnum_workers=15\u001b[0m\u001b[38;5;105m in the \u001b[33mDataLoader` to improve performance.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "config = TabularConfig(num_samples=200, num_features=8, num_classes=3, batch_size=32)\n",
    "datamodule = TabularDataModule(config)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "accelerator, devices = detect_available_accelerator()\n",
    "model = TabularClassifier(config)\n",
    "\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        max_epochs=3,\n",
    "        limit_train_batches=5,\n",
    "        limit_val_batches=2,\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=False,\n",
    "    )\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "    model.eval()\n",
    "    print(\"Training complete.\")\n",
    "except Exception as exc:\n",
    "    print(f\"Skipping training (dependency/runtime issue): {exc}\")\n",
    "    model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256c67a",
   "metadata": {},
   "source": [
    "## 2) Save and reload via `state_dict`\n",
    "\n",
    "Standard PyTorch approach; portable anywhere PyTorch runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948d07b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state_dict -> outputs/tutorials/06_model_exports/tabular_classifier.pt outputs/tutorials/06_model_exports/tabular_classifier.pt\n",
      "Reloaded model; param checksum: 2.5657803267240524 2.5657803267240524\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"outputs/tutorials/06_model_exports\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "state_path = save_dir / \"tabular_classifier.pt\"\n",
    "torch.save(model.state_dict(), state_path)\n",
    "print(\"Saved state_dict ->\", state_path)\n",
    "\n",
    "reloaded = TabularClassifier(config)\n",
    "reloaded.load_state_dict(torch.load(state_path))\n",
    "reloaded.eval()\n",
    "print(\"Reloaded model; param checksum:\", sum(p.sum().item() for p in reloaded.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3e2e86",
   "metadata": {},
   "source": [
    "## 3) Build a pure PyTorch inference module\n",
    "\n",
    "To avoid Lightning-specific attributes (e.g., `trainer`) during export, wrap the underlying MLP in a plain `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6bedf10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularInference(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TabularInference(torch.nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize inference module and load weights from the Lightning model\n",
    "inference_model = TabularInference(config.num_features, config.num_classes)\n",
    "# Filter state_dict keys that belong to the Sequential under `model`\n",
    "inference_state = {k.replace(\"model.\", \"\", 1): v for k, v in reloaded.state_dict().items() if k.startswith(\"model.\")}\n",
    "inference_model.load_state_dict(inference_state, strict=False)\n",
    "inference_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0064e60",
   "metadata": {},
   "source": [
    "## 4) Export to TorchScript for LibTorch/C++\n",
    "\n",
    "Export **both** TorchScript variants so the difference is explicit:\n",
    "\n",
    "- `torch.jit.script(model)`: compiles from Python code and preserves control flow (`if`, loops).\n",
    "- `torch.jit.trace(model, example_input)`: records ops for one example path; great for static feed-forward graphs, but it can miss data-dependent branches.\n",
    "\n",
    "In production, prefer `script` when possible; use `trace` when scripting is not supported or for simple static models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ea1daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved TorchScript -> outputs/tutorials/06_model_exports/tabular_classifier_scripted.pt outputs/tutorials/06_model_exports/tabular_classifier_scripted.pt\n",
      "TorchScript output shape: (1, 3) (1, 3)\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(1, config.num_features)\n",
    "\n",
    "# 1) Scripted TorchScript (captures model code/control flow)\n",
    "scripted_mod = torch.jit.script(inference_model)\n",
    "scripted_path = save_dir / \"tabular_classifier_scripted.pt\"\n",
    "scripted_mod.save(scripted_path)\n",
    "print(\"Saved scripted TorchScript ->\", scripted_path)\n",
    "\n",
    "# 2) Traced TorchScript (records ops from an example input)\n",
    "traced_mod = torch.jit.trace(inference_model, example_input)\n",
    "traced_path = save_dir / \"tabular_classifier_traced.pt\"\n",
    "traced_mod.save(traced_path)\n",
    "print(\"Saved traced TorchScript ->\", traced_path)\n",
    "\n",
    "# Verify round-trip load for both artifacts\n",
    "loaded_scripted = torch.jit.load(scripted_path)\n",
    "loaded_traced = torch.jit.load(traced_path)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_scripted = loaded_scripted(example_input)\n",
    "    out_traced = loaded_traced(example_input)\n",
    "\n",
    "print(\"Scripted output shape:\", tuple(out_scripted.shape))\n",
    "print(\"Traced output shape:\", tuple(out_traced.shape))\n",
    "print(\"Max |scripted - traced| on sample input:\", float((out_scripted - out_traced).abs().max().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f7281",
   "metadata": {},
   "source": [
    "## 5) Loading from C++ (LibTorch)\n",
    "\n",
    "Both exported files can be loaded from C++ via `torch::jit::load`:\n",
    "\n",
    "- `tabular_classifier_scripted.pt`\n",
    "- `tabular_classifier_traced.pt`\n",
    "\n",
    "A minimal C++ snippet:\n",
    "\n",
    "```cpp\n",
    "#include <torch/torch.h>\n",
    "#include <torch/script.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main() {\n",
    "    torch::jit::script::Module module = torch::jit::load(\"tabular_classifier_scripted.pt\");\n",
    "    std::vector<torch::jit::IValue> inputs;\n",
    "    inputs.push_back(torch::randn({1, 8})); // match num_features\n",
    "    at::Tensor output = module.forward(inputs).toTensor();\n",
    "    std::cout << output.sizes() << std::endl;\n",
    "    return 0;\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}