{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pion Stop Regression (ZenML)\n",
    "\n",
    "Train the `PionStopRegressor` on real preprocessed pion time-group data:\n",
    "- Load preprocessed pion groups from .npy files in the data folder\n",
    "- Train the `PionStopRegressor` via a ZenML pipeline with Optuna hyperparameter tuning\n",
    "- Reload artifacts and evaluate regression performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pioneerml.zenml import load_step_output\n",
    "from pioneerml.zenml import utils as zenml_utils\n",
    "from pioneerml.zenml.pipelines.training import pion_stop_optuna_pipeline\n",
    "\n",
    "PROJECT_ROOT = zenml_utils.find_project_root()\n",
    "zenml_client = zenml_utils.setup_zenml_for_notebook(root_path=PROJECT_ROOT, use_in_memory=True)\n",
    "print(f\"ZenML ready with stack: {zenml_client.active_stack_model.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the pipeline\n",
    "# Use absolute path based on project root\n",
    "file_pattern = str(Path(PROJECT_ROOT) / 'data' / 'mainTimeGroups_*.npy')\n",
    "run = pion_stop_optuna_pipeline.with_options(enable_cache=False)(\n",
    "    build_datamodule_params={\n",
    "        # Data loading parameters\n",
    "        'file_pattern': file_pattern,\n",
    "        'pion_pdg': 1,\n",
    "        'max_files': 10,\n",
    "        'limit_groups': 100000,\n",
    "        'min_hits': 3,\n",
    "        'min_pion_hits': 3,\n",
    "        # Datamodule parameters\n",
    "        'use_true_time': True,\n",
    "        'batch_size': 32,\n",
    "        'num_workers': 0,\n",
    "        'val_split': 0.15,\n",
    "        'seed': 42,\n",
    "    },\n",
    "    run_hparam_search_params={\n",
    "        'n_trials': 25,\n",
    "        'max_epochs': 20,\n",
    "        'limit_train_batches': 0.8,\n",
    "        'limit_val_batches': 1.0,\n",
    "    },\n",
    "    train_best_model_params={\n",
    "        'max_epochs': 50,\n",
    "        'early_stopping': True,\n",
    "        'early_stopping_patience': 6,\n",
    "        'early_stopping_monitor': 'val_loss',\n",
    "        'early_stopping_mode': 'min',\n",
    "    },\n",
    ")\n",
    "print(f\"Run name: {run.name}\")\n",
    "print(f\"Run status: {run.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_module = load_step_output(run, \"train_best_pion_stop_regressor\")\n",
    "datamodule = load_step_output(run, \"build_pion_stop_datamodule\")\n",
    "predictions = load_step_output(run, \"collect_pion_stop_predictions\", index=0)\n",
    "targets = load_step_output(run, \"collect_pion_stop_predictions\", index=1)\n",
    "best_params = load_step_output(run, \"run_pion_stop_hparam_search\")\n",
    "\n",
    "if trained_module is None or datamodule is None:\n",
    "    raise RuntimeError(\"Could not load artifacts from the optuna pipeline run.\")\n",
    "\n",
    "datamodule.setup(stage=\"fit\")\n",
    "trained_module.eval()\n",
    "device = next(trained_module.parameters()).device\n",
    "val_size = len(datamodule.val_dataset) if datamodule.val_dataset is not None else len(datamodule.train_dataset)\n",
    "print(f\"Loaded module on {device}; validation samples: {val_size}\")\n",
    "print(\"Best params from Optuna:\", best_params)\n",
    "print(\"Epochs actually run:\", getattr(trained_module, \"final_epochs_run\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Euclidean distances between predictions and targets\n",
    "preds_np = predictions.detach().cpu().numpy() if hasattr(predictions, \"detach\") else np.asarray(predictions)\n",
    "targets_np = targets.detach().cpu().numpy() if hasattr(targets, \"detach\") else np.asarray(targets)\n",
    "\n",
    "distances = np.linalg.norm(preds_np - targets_np, axis=1)\n",
    "mean_distance = float(distances.mean())\n",
    "median_distance = float(np.median(distances))\n",
    "\n",
    "print(f\"Mean Euclidean distance: {mean_distance:.4f} mm\")\n",
    "print(f\"Median Euclidean distance: {median_distance:.4f} mm\")\n",
    "\n",
    "# Plot distance histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].hist(distances, bins=50, range=(0, 2.0), alpha=0.75, color='teal')\n",
    "axes[0].set_xlabel('Euclidean error [mm]')\n",
    "axes[0].set_ylabel('Counts')\n",
    "axes[0].set_title('Pion stop prediction error (linear scale)')\n",
    "axes[0].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[1].hist(distances, bins=50, range=(0, 2.0), alpha=0.75, color='teal')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_xlabel('Euclidean error [mm]')\n",
    "axes[1].set_ylabel('Counts (log scale)')\n",
    "axes[1].set_title('Pion stop prediction error (log scale)')\n",
    "axes[1].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss curves\n",
    "from pioneerml.evaluation.plots import plot_loss_curves\n",
    "plot_loss_curves(trained_module, title=\"Pion stop regression: loss\", show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Trained Model\n",
    "\n",
    "Save the trained model and metadata for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Create checkpoints directory\n",
    "checkpoints_dir = Path(PROJECT_ROOT) / \"artifacts\" / \"checkpoints\" / \"pion_stop\"\n",
    "checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Generate a timestamped filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_filename = f\"pion_stop_{timestamp}.pt\"\n",
    "metadata_filename = f\"pion_stop_{timestamp}_metadata.json\"\n",
    "\n",
    "# Extract the underlying model from the Lightning module\n",
    "model = trained_module.model\n",
    "model.eval()\n",
    "\n",
    "# Save model state_dict\n",
    "model_path = checkpoints_dir / model_filename\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Saved model state_dict to: {model_path}\")\n",
    "\n",
    "# Compute evaluation metrics for metadata\n",
    "preds_np = predictions.detach().cpu().numpy() if hasattr(predictions, \"detach\") else np.asarray(predictions)\n",
    "targets_np = targets.detach().cpu().numpy() if hasattr(targets, \"detach\") else np.asarray(targets)\n",
    "distances = np.linalg.norm(preds_np - targets_np, axis=1)\n",
    "mean_distance = float(distances.mean())\n",
    "median_distance = float(np.median(distances))\n",
    "\n",
    "# Save metadata (hyperparameters, training info, etc.)\n",
    "metadata = {\n",
    "    \"model_type\": \"PionStopRegressor\",\n",
    "    \"timestamp\": timestamp,\n",
    "    \"run_name\": run.name,\n",
    "    \"best_hyperparameters\": best_params,\n",
    "    \"training_config\": getattr(trained_module, \"training_config\", {}),\n",
    "    \"epochs_run\": getattr(trained_module, \"final_epochs_run\", None),\n",
    "    \"dataset_info\": {\n",
    "        \"train_size\": len(datamodule.train_dataset) if datamodule.train_dataset else 0,\n",
    "        \"val_size\": len(datamodule.val_dataset) if datamodule.val_dataset else 0,\n",
    "        \"use_true_time\": getattr(datamodule, \"use_true_time\", None),\n",
    "    },\n",
    "    \"model_architecture\": {\n",
    "        \"hidden\": best_params.get(\"hidden\"),\n",
    "        \"heads\": best_params.get(\"heads\"),\n",
    "        \"layers\": best_params.get(\"layers\"),\n",
    "        \"dropout\": best_params.get(\"dropout\"),\n",
    "    },\n",
    "    \"evaluation_metrics\": {\n",
    "        \"mean_euclidean_distance_mm\": mean_distance,\n",
    "        \"median_euclidean_distance_mm\": median_distance,\n",
    "    },\n",
    "}\n",
    "\n",
    "metadata_path = checkpoints_dir / metadata_filename\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Saved metadata to: {metadata_path}\")\n",
    "\n",
    "print(f\"\\nModel saved successfully!\")\n",
    "print(f\"  Model: {model_path}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pioneerml)",
   "language": "python",
   "name": "pioneerml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
