{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b1537d",
   "metadata": {},
   "source": [
    "# Tutorial 0: ZenML Quickstart\n",
    "\n",
    "Run the smallest end-to-end ZenML pipeline in this repository, load the\n",
    "trained model and data module from the ZenML artifacts, and generate a few\n",
    "quick diagnostic plots.\n",
    "\n",
    "What you'll see (with detailed interpretation guidance):\n",
    "- How to spin up ZenML in in-memory mode (no server, minimal local state).\n",
    "- A minimal training run on synthetic data using our `GroupClassifier`.\n",
    "- How to pull artifacts back out of ZenML and compute plots.\n",
    "- How to interpret each plot (axes, computation, and what “good” looks like).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pioneerml.evaluation.plots import (\n",
    "    plot_multilabel_confusion_matrix,\n",
    "    plot_precision_recall_curves,\n",
    "    plot_roc_curves,\n",
    ")\n",
    "from pioneerml.training import plot_loss_curves\n",
    "from pioneerml.zenml import load_step_output\n",
    "from pioneerml.zenml import utils as zenml_utils\n",
    "from pioneerml.zenml.pipelines import zenml_training_pipeline\n",
    "\n",
    "# Initialize ZenML for notebook use\n",
    "# setup_zenml_for_notebook automatically finds the project root by searching\n",
    "# upward for .zen or .zenml directories, ensuring we use the root configuration.\n",
    "# use_in_memory=True creates a temporary in-memory SQLite store, perfect for\n",
    "# tutorials where we don't need persistent artifact storage.\n",
    "zenml_client = zenml_utils.setup_zenml_for_notebook(use_in_memory=True)\n",
    "print(f\"ZenML initialized with stack: {zenml_client.active_stack_model.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2c85e3",
   "metadata": {},
   "source": [
    "## Run the Training Pipeline\n",
    "\n",
    "Here we execute the complete ZenML training pipeline. The pipeline consists of\n",
    "several steps:\n",
    "\n",
    "1. **build_datamodule**: Creates synthetic graph data and splits it into train/val sets\n",
    "2. **build_module**: Instantiates the GroupClassifier model wrapped in a Lightning module\n",
    "3. **train_module**: Trains the model using PyTorch Lightning (auto-detects CPU/GPU)\n",
    "4. **collect_predictions**: Runs inference on the validation set to get predictions and targets\n",
    "\n",
    "**Why use `enable_cache=False`?** This ensures the pipeline runs fresh each time,\n",
    "which is useful for tutorials. In production, you'd typically enable caching to\n",
    "skip re-running unchanged steps.\n",
    "\n",
    "After the pipeline completes, we load the artifacts (trained model, datamodule,\n",
    "predictions, targets) using `load_step_output`. These artifacts are stored by\n",
    "ZenML and can be reloaded anytime without re-running the pipeline - this makes\n",
    "notebooks fast and interactive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0696cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = zenml_training_pipeline.with_options(enable_cache=False)()\n",
    "print(f\"Pipeline run status: {run.status}\")\n",
    "\n",
    "trained_module = load_step_output(run, \"train_module\")\n",
    "datamodule = load_step_output(run, \"build_datamodule\")\n",
    "predictions = load_step_output(run, \"collect_predictions\")[0]\n",
    "targets = load_step_output(run, \"collect_predictions\")[1]\n",
    "\n",
    "if trained_module is None or datamodule is None:\n",
    "    raise RuntimeError(\"Could not load artifacts from the zenml_training_pipeline run.\")\n",
    "\n",
    "trained_module.eval()\n",
    "datamodule.setup(stage=\"fit\")\n",
    "device = next(trained_module.parameters()).device\n",
    "print(f\"Loaded artifacts from run {run.name} (device={device})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9fba9",
   "metadata": {},
   "source": [
    "## Verify Predictions Were Collected\n",
    "\n",
    "The `collect_predictions` step in the pipeline has already run inference and\n",
    "collected predictions and targets. This cell simply verifies how many samples\n",
    "were processed. The predictions are raw logits (before sigmoid), and targets\n",
    "are one-hot encoded class labels - both ready for evaluation metrics and plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41fdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Collected predictions for {len(targets)} samples via pipeline step.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274faf19",
   "metadata": {},
   "source": [
    "## Visualize Training Diagnostics\n",
    "\n",
    "We generate four diagnostic plots to understand model performance. All plots are\n",
    "displayed inline in the notebook (no files saved) by setting `show=True` and\n",
    "`save_path=None`. This makes the notebook self-contained and easy to share.\n",
    "\n",
    "**1. Loss Curves** - Shows training and validation loss over epochs\n",
    "- **What it shows**: How well the model is learning during training\n",
    "- **Good signs**: Both curves decrease steadily and stay close together\n",
    "- **Warning signs**: Large gap between train/val (overfitting), or flat lines (not learning)\n",
    "\n",
    "**2. Confusion Matrices** - Per-class classification accuracy\n",
    "- **What it shows**: For each class (π, μ, e+), how many true positives, false positives,\n",
    "  true negatives, and false negatives\n",
    "- **Good signs**: Dark diagonal (correct predictions), light off-diagonal (few errors)\n",
    "- **Why normalized**: Makes it easy to compare classes with different sample sizes\n",
    "\n",
    "**3. ROC Curves** - Ranking quality across all possible thresholds\n",
    "- **What it shows**: True Positive Rate vs False Positive Rate as we vary the\n",
    "  classification threshold\n",
    "- **AUC score**: Area under curve - higher is better (1.0 = perfect, 0.5 = random)\n",
    "- **Good signs**: Curves in top-left corner, AUC > 0.8\n",
    "\n",
    "**4. Precision-Recall Curves** - Performance on imbalanced data\n",
    "- **What it shows**: Precision vs Recall trade-off as we vary the threshold\n",
    "- **Average Precision**: Summarizes performance, especially important for imbalanced classes\n",
    "- **Good signs**: High curves that maintain precision even at high recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(trained_module, title=\"Quickstart: Loss Curves\", show=True)\n",
    "\n",
    "plot_multilabel_confusion_matrix(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    class_names=[\"pi\", \"mu\", \"e+\"],\n",
    "    threshold=0.5,\n",
    "    normalize=True,\n",
    "    save_path=None,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "plot_roc_curves(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    class_names=[\"pi\", \"mu\", \"e+\"],\n",
    "    save_path=None,\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "plot_precision_recall_curves(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    class_names=[\"pi\", \"mu\", \"e+\"],\n",
    "    save_path=None,\n",
    "    show=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
