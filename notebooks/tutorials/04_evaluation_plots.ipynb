{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7579b290",
   "metadata": {},
   "source": [
    "# Tutorial 4: Evaluation and Custom Plots\n",
    "\n",
    "Train a small model, gather predictions, compute metrics, and save standard\n",
    "evaluation plots using the built-in plotting utilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05737df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from pioneerml.zenml import load_step_output\n",
    "from pioneerml.zenml import utils as zenml_utils\n",
    "from pioneerml.zenml.pipelines import evaluation_examples_pipeline\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "else:\n",
    "    PROJECT_ROOT = Path.cwd().resolve()\n",
    "\n",
    "zenml_client = zenml_utils.setup_zenml_for_notebook(root_path=PROJECT_ROOT, use_in_memory=True)\n",
    "print(f\"ZenML stack: {zenml_client.active_stack_model.name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54058d0",
   "metadata": {},
   "source": [
    "## Run the evaluation pipeline\n",
    "The pipeline trains a model, collects predictions on the validation split, and\n",
    "generates a few standard plots (confusion matrices, ROC, PR).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = evaluation_examples_pipeline.with_options(enable_cache=False)()\n",
    "print(f\"Pipeline run {run.name} status: {run.status}\")\n",
    "\n",
    "trained_module = load_step_output(run, \"train_evaluation_model\")\n",
    "datamodule = load_step_output(run, \"prepare_evaluation_datamodule\")\n",
    "predictions_and_targets = load_step_output(run, \"collect_predictions\")\n",
    "metrics = load_step_output(run, \"compute_custom_metrics\")\n",
    "plot_paths = load_step_output(run, \"generate_evaluation_plots\")\n",
    "\n",
    "if trained_module is None or datamodule is None:\n",
    "    raise RuntimeError(\"Could not load required artifacts from the evaluation_examples_pipeline run.\")\n",
    "\n",
    "trained_module.eval()\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261385e5",
   "metadata": {},
   "source": [
    "## Pull predictions and metrics\n",
    "Load the predictions/targets from the ZenML artifact (fallback to recomputing\n",
    "them if needed), then print the summary metrics returned by the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f25f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = next(trained_module.parameters()).device\n",
    "\n",
    "if predictions_and_targets is not None:\n",
    "    predictions, targets = predictions_and_targets\n",
    "else:\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "    if isinstance(val_loader, list) and len(val_loader) == 0:\n",
    "        val_loader = datamodule.train_dataloader()\n",
    "\n",
    "    preds, trgs = [], []\n",
    "    for batch in val_loader:\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds.append(trained_module(batch).detach().cpu())\n",
    "            trgs.append(batch.y.detach().cpu())\n",
    "    predictions = torch.cat(preds)\n",
    "    targets = torch.cat(trgs)\n",
    "\n",
    "print(\"Metrics:\")\n",
    "if metrics:\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"- {key}: {value}\")\n",
    "else:\n",
    "    print(\"- No metrics artifact found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22534c21",
   "metadata": {},
   "source": [
    "## Plot locations\n",
    "The evaluation step already saved plots to disk. Surface the paths here so\n",
    "they are easy to find from the notebook or CLI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73449161",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_paths:\n",
    "    print(\"Plot paths:\")\n",
    "    for name, path in plot_paths.items():\n",
    "        print(f\"- {name}: {path}\")\n",
    "else:\n",
    "    print(\"No plot paths were recorded.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}