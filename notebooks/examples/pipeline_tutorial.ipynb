{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca49c149",
   "metadata": {},
   "source": [
    "# PIONEER ML Tutorial\n",
    "\n",
    "Quick walkthrough of the pipeline framework, Lightning utilities, and how to plug in a model. This notebook builds a tiny synthetic dataset, wraps it in a DataModule, trains a model with the Lightning pipeline stage, and inspects results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40f0fc",
   "metadata": {},
   "source": [
    "## 1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e7e898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: /home/jack/python_projects/pioneerML\n",
      "Added to sys.path: /home/jack/python_projects/pioneerML/src\n"
     ]
    }
   ],
   "source": [
    "import sys, pathlib\n",
    "\n",
    "def _find_repo_root(start: pathlib.Path) -> pathlib.Path:\n",
    "    for path in [start, *start.parents]:\n",
    "        if (path / 'pyproject.toml').exists():\n",
    "            return path\n",
    "    return start\n",
    "\n",
    "repo_root = _find_repo_root(pathlib.Path.cwd())\n",
    "src_dir = repo_root / 'src'\n",
    "for candidate in (repo_root, src_dir):\n",
    "    if str(candidate) not in sys.path:\n",
    "        sys.path.insert(0, str(candidate))\n",
    "\n",
    "print('Added to sys.path:', repo_root)\n",
    "print('Added to sys.path:', src_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396ac4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pioneerml.data import GraphGroupDataset\n",
    "from pioneerml.models import GroupClassifier\n",
    "from pioneerml.pipelines import Pipeline, Context, StageConfig\n",
    "from pioneerml.pipelines.stages import LightningTrainStage\n",
    "from pioneerml.training import GraphDataModule, GraphLightningModule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b41dc3",
   "metadata": {},
   "source": [
    "## 2) Create a synthetic dataset\n",
    "We generate a few fake time-group records with the standardized per-hit fields expected by `GraphGroupDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fccf8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[8, 5], edge_index=[2, 56], edge_attr=[56, 4], y=[3], event_id=0, group_id=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_record(num_hits: int, event_id: int) -> dict:\n",
    "    coord = np.random.randn(num_hits).astype(np.float32)\n",
    "    z = np.random.randn(num_hits).astype(np.float32)\n",
    "    energy = np.abs(np.random.randn(num_hits)).astype(np.float32)\n",
    "    view = np.random.randint(0, 2, num_hits).astype(np.float32)\n",
    "\n",
    "    # Multi-label targets: [pion, muon, mip]\n",
    "    labels = [int(energy.mean() > 0.5), int(num_hits % 2 == 0)]\n",
    "    if len(labels) < 3:\n",
    "        labels.append(0)\n",
    "\n",
    "    return {\n",
    "        \"coord\": coord,\n",
    "        \"z\": z,\n",
    "        \"energy\": energy,\n",
    "        \"view\": view,\n",
    "        \"labels\": labels,\n",
    "        \"event_id\": event_id,\n",
    "        \"group_id\": event_id,\n",
    "    }\n",
    "\n",
    "records = [make_record(num_hits=8 + i, event_id=i) for i in range(20)]\n",
    "dataset = GraphGroupDataset(records, num_classes=3)\n",
    "dataset[0]  # trigger a build and inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a5ed79",
   "metadata": {},
   "source": [
    "## 3) Wrap data with a Lightning DataModule\n",
    "Splits the dataset and prepares PyG loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da12c2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch_geometric.loader.dataloader.DataLoader at 0x7856a6525190>,\n",
       " <torch_geometric.loader.dataloader.DataLoader at 0x7856a5b5fe50>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule = GraphDataModule(dataset=dataset, batch_size=4, val_split=0.2, test_split=0.0, num_workers=2)\n",
    "datamodule.setup()\n",
    "datamodule.train_dataloader(), datamodule.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e2f222",
   "metadata": {},
   "source": [
    "## 4) Build the model and Lightning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669b3de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphLightningModule(\n",
       "  (model): GroupClassifier(\n",
       "    (input_embed): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x FullGraphTransformerBlock(\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): TransformerConv(64, 16, heads=4)\n",
       "        (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (jk): JumpingKnowledge(cat)\n",
       "    (pool): AttentionalAggregation(gate_nn=Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "    ), nn=None)\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Linear(in_features=64, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GroupClassifier(num_classes=3, hidden=64, num_blocks=2)\n",
    "lightning_module = GraphLightningModule(model, task=\"classification\", lr=1e-3)\n",
    "lightning_module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d8425",
   "metadata": {},
   "source": [
    "## 5) Compose a pipeline with the Lightning training stage\n",
    "The `LightningTrainStage` fits the module using any provided DataModule and records the trainer + trained module back into the shared `Context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93b2d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type              | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | model   | GroupClassifier   | 118 K  | train\n",
      "1 | loss_fn | BCEWithLogitsLoss | 0      | train\n",
      "------------------------------------------------------\n",
      "118 K     Trainable params\n",
      "0         Non-trainable params\n",
      "118 K     Total params\n",
      "0.472     Total estimated model params size (MB)\n",
      "47        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a865067ceaa41448adb36738eed7cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 81. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05cd635ab9b24e9dbf72260744e3e647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 62. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 72. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb59b706774e4c8d9a7b5faa0c1d3dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 53. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/home/jack/miniconda3/envs/pioneerml-uv/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 65. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3403c483dac4f0c950234575fa461a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.22440795600414276,\n",
       " 'val_accuracy': 1.0,\n",
       " 'train_loss': 0.4237568974494934,\n",
       " 'train_accuracy': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stage = LightningTrainStage(\n",
    "    config=StageConfig(\n",
    "        name=\"train\",\n",
    "        params={\n",
    "            \"module\": lightning_module,\n",
    "            \"datamodule\": datamodule,\n",
    "            \"trainer_params\": {\n",
    "                \"max_epochs\": 2,\n",
    "                \"limit_train_batches\": 2,\n",
    "                \"limit_val_batches\": 1,\n",
    "                \"logger\": False,\n",
    "                \"enable_checkpointing\": False,\n",
    "                \"accelerator\": \"gpu\",   # required for CUDA\n",
    "                \"devices\": 1,           # pick GPU 0\n",
    "            },\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([train_stage], name=\"tutorial_pipeline\")\n",
    "ctx = pipeline.run(Context())\n",
    "ctx.summary()\n",
    "ctx.get(\"metrics\", {})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cee97",
   "metadata": {},
   "source": [
    "## 6) Next steps\n",
    "- Swap in your own datasets or DataModules.\n",
    "- Add stages for preprocessing, evaluation, and checkpointing.\n",
    "- Integrate experiment tracking (e.g., Weights & Biases) by configuring the Lightning Trainer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}