{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Splitter Event\n",
    "\n",
    "Train the event-level edge-affinity splitter with classifier/splitter/endpoint priors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pioneerml.common.evaluation.plots.loss import LossCurvesPlot\n",
    "from pioneerml.common.zenml import load_step_output\n",
    "from pioneerml.common.zenml import utils as zenml_utils\n",
    "from pioneerml.pipelines.training import event_splitter_event_pipeline\n",
    "\n",
    "PROJECT_ROOT = zenml_utils.find_project_root()\n",
    "zenml_utils.setup_zenml_for_notebook(root_path=PROJECT_ROOT, use_in_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build aligned inputs: main parquet + group classifier + group splitter + endpoint predictions\n",
    "data_dir = Path(PROJECT_ROOT) / \"data\"\n",
    "main_paths = sorted(data_dir.glob(\"ml_output_*.parquet\"))\n",
    "group_probs_dir = data_dir / \"inference_outputs\" / \"group_classifier_event\"\n",
    "splitter_probs_dir = data_dir / \"inference_outputs\" / \"group_splitter_event\"\n",
    "endpoint_dir = data_dir / \"inference_outputs\" / \"endpoint_regressor_event\"\n",
    "\n",
    "def _pick_pred(path_dir: Path, main_path: Path) -> Path | None:\n",
    "    latest = path_dir / main_path.name.replace(\".parquet\", \"_preds_latest.parquet\")\n",
    "    if latest.exists():\n",
    "        return latest\n",
    "    return None\n",
    "\n",
    "paired = []\n",
    "missing = []\n",
    "for main_path in main_paths:\n",
    "    group_path = _pick_pred(group_probs_dir, main_path)\n",
    "    splitter_path = _pick_pred(splitter_probs_dir, main_path)\n",
    "    endpoint_path = _pick_pred(endpoint_dir, main_path)\n",
    "    if group_path is not None and splitter_path is not None and endpoint_path is not None:\n",
    "        paired.append((str(main_path), str(group_path), str(splitter_path), str(endpoint_path)))\n",
    "    else:\n",
    "        missing.append(str(main_path))\n",
    "\n",
    "if not paired:\n",
    "    raise RuntimeError(\n",
    "        \"No aligned main/group/splitter/endpoint parquet quartets found. \"\n",
    "        \"Run prior-stage inference first.\"\n",
    "    )\n",
    "if missing:\n",
    "    print(f\"Warning: missing predictions for {len(missing)} shard(s); skipping those files.\")\n",
    "\n",
    "parquet_paths = [p[0] for p in paired]\n",
    "group_probs_parquet_paths = [p[1] for p in paired]\n",
    "group_splitter_parquet_paths = [p[2] for p in paired]\n",
    "endpoint_parquet_paths = [p[3] for p in paired]\n",
    "print(f\"Using {len(parquet_paths)} shard(s) for event-splitter-event training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cfg = {\n",
    "    \"loader\": {\n",
    "        \"config_json\": {\n",
    "            \"time_window_ns\": 1.0,\n",
    "            \"use_group_probs\": True,\n",
    "            \"use_splitter_probs\": True,\n",
    "            \"use_endpoint_preds\": True,\n",
    "        },\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    \"hpo\": {\n",
    "        \"enabled\": True,\n",
    "        \"n_trials\": 1,\n",
    "        \"max_epochs\": 3,\n",
    "        \"storage\": f\"sqlite:///{PROJECT_ROOT}/.optuna/event_splitter_event_hpo.db\",\n",
    "    },\n",
    "    \"train\": {\"max_epochs\": 3},\n",
    "    \"evaluate\": {\"threshold\": 0.5, \"batch_size\": 1},\n",
    "    \"export\": {\n",
    "        \"prefer_cuda\": True,\n",
    "        \"export_dir\": str(Path(PROJECT_ROOT) / \"trained_models\" / \"event_splitter_event\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "run = event_splitter_event_pipeline.with_options(enable_cache=False)(\n",
    "    parquet_paths=parquet_paths,\n",
    "    group_probs_parquet_paths=group_probs_parquet_paths,\n",
    "    group_splitter_parquet_paths=group_splitter_parquet_paths,\n",
    "    endpoint_parquet_paths=endpoint_parquet_paths,\n",
    "    pipeline_config=pipeline_cfg,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "def _concise(values, limit: int = 10):\n",
    "    values = list(values)\n",
    "    return values[-limit:] if len(values) > limit else values\n",
    "\n",
    "trained_module = load_step_output(run, \"train_event_splitter_event\")\n",
    "hpo_params = load_step_output(run, \"tune_event_splitter_event\")\n",
    "metrics = load_step_output(run, \"evaluate_event_splitter_event\")\n",
    "export = load_step_output(run, \"export_event_splitter_event\")\n",
    "print(\"hpo_params:\", hpo_params)\n",
    "if trained_module is not None:\n",
    "    print(\"train_epoch_loss_history:\", _concise(trained_module.train_epoch_loss_history))\n",
    "    print(\"val_epoch_loss_history:\", _concise(trained_module.val_epoch_loss_history))\n",
    "print(\"metrics:\", metrics)\n",
    "print(\"export:\", export)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "if trained_module is None:\n",
    "    raise RuntimeError(\"No trained module loaded from pipeline run.\")\n",
    "LossCurvesPlot().render(trained_module, show=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}